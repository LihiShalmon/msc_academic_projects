{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LihiShalmon/msc_academic_projects/blob/main/PS1_Part2_Hello_NN_2024_ID_208747287.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuoXusr9B3nd"
      },
      "source": [
        "# PS1: Your first library-free neural network!  \n",
        "\n",
        "Advanced Learning 2024\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRYKvBtcZj44"
      },
      "source": [
        "For SUBMISSION:   \n",
        "\n",
        "Please upload the complete and executed `ipynb` to your git repository. Verify that all of your output can be viewed directly from github, and provide a link to that git file below.\n",
        "\n",
        "~~~\n",
        "STUDENT ID: 208747287\n",
        "~~~\n",
        "\n",
        "~~~\n",
        "STUDENT GIT LINK: [GITHUB](https://github.com/LihiShalmon/BigDataFinalProject/blob/main/PS1_Part2_Hello_NN_2024_ID_208747287.ipynb)\n",
        "~~~\n",
        "In Addition, don't forget to add your ID to the files:    \n",
        "  \n",
        "`PS1_Part2_HelloNN_2024_ID_[000000000].html`   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "X2ZxCWBO_IIT"
      },
      "outputs": [],
      "source": [
        "import numpy as np # You are allowed to use  only numpy.\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnAz84ZL_9XQ"
      },
      "source": [
        "\n",
        "**Welcome**.   \n",
        "\n",
        "In this part of the problem set you are set to build a complete and flexible neural network.  \n",
        "This neural network will be library free (in the sense that we won't use PyTorch/Tensorflow/etc.).   \n",
        "\n",
        "Let's do a quick review of the basic neural-network components:  \n",
        "\n",
        "\n",
        "*   *Layer* - can be fully connected (dense/hidden), convolution, etc.\n",
        "  * Forward propagation- the layer outputs the next layer's input\n",
        "  * Backward propagation- the layer also outputs the gradient descent update\n",
        "*   *Activation* Layer (e.g. ReLU) - there are no parameters, only gradients with respect to the input. We want to compute both the gradient w.r.t the parameters of the layer and to create the gradient with respect to the layer's inputs\n",
        "   * *Forward propagation*- the layer outputs the next layer's input\n",
        "   * *Backward propagation*- the layer also outputs the gradient descent update\n",
        "*   *Loss Function* : how our model  quantifies the difference between the predicted outputs the actual (target) values  \n",
        "*   *Network Wrapper*-  wraps our components together as a trainable model.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncQsamml85JG"
      },
      "source": [
        "Useful resource:  \n",
        "* Gradient descent for neural networks [cheat sheet](https://moodle4.cs.huji.ac.il/hu23/mod/resource/view.php?id=402297).\n",
        "* Neural network architecture [cheat sheet](https://moodle4.cs.huji.ac.il/hu23/mod/url/view.php?id=402298)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdK5HvttdcRW",
        "outputId": "c6bdf3d6-6123-4e98-bc20-924e2bffc9cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P11k0GECXiR-"
      },
      "source": [
        "### 0. Loading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwLDOo7IXfcI"
      },
      "source": [
        "You are going to test and evaluate your home-made network on the `mnist` dataset.   \n",
        "The MNIST dataset is a large dataset of handwritten digits that is commonly used for training various image and vision models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "RIxpddzDXgBN"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "# load MNIST from server\n",
        "# Using a standard library (keras.datasets) to load the mnist data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_yaFDAtXj1h"
      },
      "source": [
        "#### Data transformations\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "3PooYSGAgY4v"
      },
      "outputs": [],
      "source": [
        "# training data : 60000 samples\n",
        "# reshape and normalize input data\n",
        "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255\n",
        "# One-hot encoding of the output.\n",
        "# Currently a number in range [0,9]; Change into a vector of size 10\n",
        "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "y_train = to_categorical(y_train)\n",
        "# same for test data : 10000 samples\n",
        "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8HGS8h1uXAD"
      },
      "source": [
        "### 1. Network's Components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hda7HDt6NMg1"
      },
      "source": [
        "Please fill-in the missing code in the code boxes below (only where  `#### SOLUTION REQUIRED ####` is specified).   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "zi3-57RaG-YW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# This class is a general layer primitive, defining that each instance must\n",
        "# have an (input,output) parameters, and 2 functions: forward+backward propogation\n",
        "class Layer_Primitive:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    # computes the output Y of a layer for a given input X\n",
        "    def forward_propagation(self, input):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hF0oGoJQdJzd"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFrfWvZFoxGz"
      },
      "source": [
        "#### Fully Connected Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZnNmnfjDqBg"
      },
      "source": [
        "A fully-connected layer (a.k.a. affine, dense,linear layer) connects every input neuron to every output neuron.   \n",
        "It has 2 parameters: (input, output).   \n",
        "You need to define (code) the following:\n",
        "* its initialization weights with random weights.\n",
        "* the forward propogation calculation (as shown in class).\n",
        "* the backward propogation gradients calculation (given output, as shown in class).\n",
        "\n",
        "Parameters must be intitialized with some values. There are many ways to initialize the weights, and you are encouraged to do a quick research about the common methods. Any commonly used method will be accepted.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2vegoAGNSdm"
      },
      "source": [
        "1.1 (20 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "1oPtObGHL0qA"
      },
      "outputs": [],
      "source": [
        "#### SOLUTION REQUIRED ####\n",
        "\n",
        "\n",
        "# inherit from base class Layer\n",
        "class Affine_Layer(Layer_Primitive):\n",
        "    # input_size = number of input neurons\n",
        "    # output_size = number of output neurons\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.randn(input_size, output_size) * 0.01  # Random initialization (small values)\n",
        "        # Randomly initialize the bias of the layer\n",
        "        self.bias = np.zeros((1, output_size))  # Biases initialized to zeros\n",
        "\n",
        "    # returns output for a given input\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = np.dot(self.input, self.weights) + self.bias\n",
        "        return self.output\n",
        "\n",
        "\n",
        "\n",
        "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
        "    def backward_propagation(self, output_grad, learning_rate):\n",
        "        input_error = np.dot(output_grad, self.weights.T)  # dE/dX = dE/dY . W^T\n",
        "        weights_error = np.dot(self.input.T, output_grad)  # dE/dW = X^T . dE/dY\n",
        "        bias_error = np.sum(output_grad, axis=0, keepdims=True)  # dE/dB = sum(dE/dY)\n",
        "\n",
        "        # Update weights and biases using gradient descent\n",
        "        self.weights -= learning_rate * weights_error\n",
        "        self.bias -= learning_rate * bias_error\n",
        "\n",
        "        return input_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uktf9H2UuhYR"
      },
      "source": [
        "#### Activation layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbjwalPGEgLy"
      },
      "source": [
        "Activation functions are often a non-linear functions that aid in how well the network model adapts to and learns  the training dataset. The choice of activation function in the output layer will define the type of predictions the model can make.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "Mg5f_-ikVMzi"
      },
      "outputs": [],
      "source": [
        "# inherit from base class Layer\n",
        "class ActivationLayer(Layer_Primitive):\n",
        "    def __init__(self, activation, activation_grad):\n",
        "        self.activation = activation\n",
        "        self.activation_grad = activation_grad\n",
        "\n",
        "    # returns the activated input\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = self.activation(self.input)\n",
        "        return self.output\n",
        "\n",
        "    # Returns input_error=dE/dX for a given output_grad=dE/dY.\n",
        "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
        "    def backward_propagation(self, output_grad, learning_rate):\n",
        "        return self.activation_grad(self.input) * output_grad\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaB7AX-aFP1j"
      },
      "source": [
        "\n",
        "You need to define (code) the following via different functions:\n",
        "* the forward propogation calculation (as shown in class).\n",
        "* the backward propogation gradients calculation (given output, as shown in class)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbvilWixNaro"
      },
      "source": [
        "1.2 (20 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "4-M9-LPBgBTB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation functions and their derivatives:\n",
        "def tanh(x):\n",
        "    pos_exp = np.exp(x)\n",
        "    neg_exp = np.exp(-x)\n",
        "    return (pos_exp - neg_exp) / (pos_exp + neg_exp)\n",
        "\n",
        "def tanh_grad(x):\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_grad(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def sigmoid(x):\n",
        "    x = np.clip(x, -500, 500)  # Stabilize\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "# Additional functions (not required to implement them)\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "def softmax_grad(softmax_output):\n",
        "    s = softmax_output.reshape(-1, 1)\n",
        "    return np.diagflat(s) - np.dot(s, s.T)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3RBktf7uowi"
      },
      "source": [
        "#### Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgZ4SRFkG_Sj"
      },
      "source": [
        "1.3 (10 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "uo_FcJrYgQaB"
      },
      "outputs": [],
      "source": [
        "#### SOLUTION REQUIRED ####\n",
        "\n",
        "# loss function and its derivative (with respect to y_pred, which we want to optimize)\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "\n",
        "    return np.mean((y_pred - y_true )**2)\n",
        "\n",
        "def mse_grad(y_true, y_pred):\n",
        "    #               scalar @ 2@ error\n",
        "    return (1/y_true.shape[0])*2*(y_pred- y_true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4jPsOT9uy-_"
      },
      "source": [
        "#### Putting everything together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEK-tOfrNhO_"
      },
      "source": [
        "1.4 (10 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "fMwSnK5pgV9Y"
      },
      "outputs": [],
      "source": [
        "#### SOLUTION REQUIRED (in `predict`) ####\n",
        "\n",
        "class MyNetwork:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.loss = None\n",
        "        self.loss_grad = None\n",
        "        self.loss_history = []\n",
        "\n",
        "    # add layer to network\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    # set loss to use\n",
        "    def use_loss(self, loss, loss_grad):\n",
        "        self.loss = loss\n",
        "        self.loss_grad = loss_grad\n",
        "\n",
        "\n",
        "    # train the network\n",
        "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
        "        # sample dimension first\n",
        "        samples = len(x_train)\n",
        "\n",
        "        # training loop\n",
        "        for i in range(epochs):\n",
        "            err = 0\n",
        "            for j in range(samples):\n",
        "                # forward propagation\n",
        "                output = x_train[j]\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forward_propagation(output)\n",
        "\n",
        "                # compute loss (for display purpose only)\n",
        "                err += self.loss(y_train[j], output)\n",
        "\n",
        "                # backward propagation\n",
        "                grad = self.loss_grad(y_train[j], output)\n",
        "                for layer in reversed(self.layers):\n",
        "                    grad = layer.backward_propagation(grad, learning_rate)\n",
        "\n",
        "            # calculate average error on all samples\n",
        "            err /= samples\n",
        "            self.loss_history.append(err)  # Store loss\n",
        "            print('Training epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
        "\n",
        "\n",
        "    # predict output for given input\n",
        "    def predict(self, x_test,y_test=np.array([])):\n",
        "        if y_test.size:\n",
        "           assert len(x_test)==len(y_test) # if Y is given\n",
        "        # sample dimension first\n",
        "        samples = len(x_test)\n",
        "        result = []\n",
        "        loss = 0\n",
        "        correct = 0\n",
        "        # run network over all samples\n",
        "        for i in range(samples):\n",
        "            # forward propagation\n",
        "            output = x_test[i]\n",
        "            for layer in self.layers:\n",
        "                output = layer.forward_propagation(output)\n",
        "            result.append(output)\n",
        "            # ONLY IF LABELS ARE GIVEN (Y):\n",
        "            if y_test.size:\n",
        "                # Evaluate the output against Y,\n",
        "                # calculate loss against Y, add to `loss`:\n",
        "                loss += mse(y_test, result)# FILL IN THE MISSING CODE\n",
        "                target = y_test[i]\n",
        "                # Evaluate the label of the output against real, and if identical (for categorical)\n",
        "                if np.argmax(output) == np.argmax(target):\n",
        "                   correct += 1\n",
        "        if y_test.size:\n",
        "            mean_loss = loss/samples\n",
        "\n",
        "            print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.\n",
        "                  format(mean_loss, correct, samples,100. * correct / samples))\n",
        "\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCXXcALGXSRb"
      },
      "source": [
        "## 2. Testing Your Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KotuuqWKXt2r"
      },
      "source": [
        "### Defining our main neural network architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzatylPzQIdR"
      },
      "source": [
        "Define your network's architecture:  \n",
        "(Please rationalize your choice of activation funciton.)\n",
        "* first affine layer that takes your input and outputs 128 nodes\n",
        "* `tanh/relu/sigmoid` activation layer following the first affine layer\n",
        "* second affine layer that takes the first layer's input and outputs 64 nodes\n",
        "* `tanh/relu/sigmoid` activation layer following the second affine layer\n",
        "* third affine layer that takes your second layer's input and outputs nodes in the size of the Y labels.\n",
        "* `tanh/relu/sigmoid` activation layer following the last affine layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSwlLJXWNqii"
      },
      "source": [
        "2.1 (5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "_O9Mi5Qmuvlp"
      },
      "outputs": [],
      "source": [
        "#### SOLUTION REQUIRED (in `predict`) ####\n",
        "\n",
        "# Network Architecture\n",
        "net = MyNetwork()\n",
        "#  maps 784 (the dimensions of 28x28 image) to 128\n",
        "net.add(Affine_Layer(input_size=784, output_size=128)) # First affine layer: 784 → 128\n",
        "net.add(ActivationLayer(activation=relu, activation_grad=relu_grad))\n",
        "\n",
        "#  maps 128 to 64\n",
        "net.add(Affine_Layer(input_size=128, output_size=64))\n",
        "net.add(ActivationLayer(activation=relu, activation_grad=relu_grad))\n",
        "\n",
        "# maps the previous input to the labels class . we have 10 different classes since mnist classifys each image to the numbers 1-10.\n",
        "net.add(Affine_Layer(input_size=64, output_size=10))\n",
        "net.add(ActivationLayer(activation=sigmoid, activation_grad=sigmoid_grad))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8_5gnOuuxWC"
      },
      "source": [
        "### Training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhWuBFBfg3SB",
        "outputId": "8fdc4797-3b5a-4323-c617-b8de2ff029e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 1/20   error=0.097914\n",
            "Training epoch 2/20   error=0.089171\n",
            "Training epoch 3/20   error=0.069539\n",
            "Training epoch 4/20   error=0.038365\n",
            "Training epoch 5/20   error=0.021967\n",
            "Training epoch 6/20   error=0.016952\n",
            "Training epoch 7/20   error=0.014473\n",
            "Training epoch 8/20   error=0.012663\n",
            "Training epoch 9/20   error=0.011238\n",
            "Training epoch 10/20   error=0.010083\n",
            "Training epoch 11/20   error=0.009125\n",
            "Training epoch 12/20   error=0.008327\n",
            "Training epoch 13/20   error=0.007648\n",
            "Training epoch 14/20   error=0.007066\n",
            "Training epoch 15/20   error=0.006562\n",
            "Training epoch 16/20   error=0.006126\n",
            "Training epoch 17/20   error=0.005740\n",
            "Training epoch 18/20   error=0.005401\n",
            "Training epoch 19/20   error=0.005096\n",
            "Training epoch 20/20   error=0.004826\n",
            "Total process time: 1096.663\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# While developing, it is recommended to train your model on a subset of the data... / or low epochs.\n",
        "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
        "net.use_loss(mse, mse_grad)\n",
        "epoch_num = 20\n",
        "lr = 0.01\n",
        "t1 = time.time()\n",
        "net.fit(x_train, y_train, epochs=epoch_num, learning_rate=lr)\n",
        "print(f\"Total process time: {round(time.time() - t1,3)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the loss\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(net.loss_history) + 1), net.loss_history, marker='o', label=\"Training Loss\")\n",
        "plt.title(\"Training Loss Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "xS8kdWeWjgGY",
        "outputId": "f43a7acd-e61a-4a11-b76d-79cbc0017f39"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAIjCAYAAADvI7a6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABEWklEQVR4nO3dd3hUVf7H8c+kJ0ASQkkI0mUhFFFBIEpRCU1UwCgYUYqsrCuoK+ACijRlWawgKKyrggWlqdgQiQg/FUNXpItKE0hCC0EDyZCc3x+czDqmEOOkDLxfzzOPzrnnznzv/Q748ebMjcMYYwQAAABAPmVdAAAAAFBeEI4BAAAAi3AMAAAAWIRjAAAAwCIcAwAAABbhGAAAALAIxwAAAIBFOAYAAAAswjEAAABgEY4BXFAGDhyounXrFmvfCRMmyOFweLYg4DxyP3dHjx4t61IAiHAMoJQ4HI4iPVatWlXWpZaJgQMHqmLFimVdRpEYY/TGG2+oQ4cOCg8PV0hIiJo3b65Jkybp119/Levy8sgNnwU9kpOTy7pEAOWIX1kXAODi8MYbb7g9f/3115WYmJhnPCYm5k+9z3//+1/l5OQUa9+xY8dq9OjRf+r9L3TZ2dm64447tHDhQrVv314TJkxQSEiIvvzyS02cOFGLFi3SZ599psjIyLIuNY9Zs2bl+z8g4eHhpV8MgHKLcAygVNx5551uz9esWaPExMQ847+XkZGhkJCQIr+Pv79/seqTJD8/P/n58ddiYZ588kktXLhQI0eO1FNPPeUaHzJkiPr06aNevXpp4MCB+uSTT0q1rqJ8Tm699VZVrVq1lCoC4K1YVgGg3Lj22mvVrFkzbdy4UR06dFBISIgeeeQRSdL777+vHj16KDo6WoGBgWrQoIEef/xxZWdnu73G79cc7927Vw6HQ08//bReeuklNWjQQIGBgbrqqqu0fv16t33zW3PscDg0bNgwLVmyRM2aNVNgYKCaNm2qZcuW5al/1apVatWqlYKCgtSgQQP95z//8fg65kWLFqlly5YKDg5W1apVdeedd+rgwYNuc5KTkzVo0CBdcsklCgwMVI0aNdSzZ0/t3bvXNWfDhg3q2rWrqlatquDgYNWrV0933313oe99+vRpPfXUU/rLX/6iKVOm5Nl+0003acCAAVq2bJnWrFkjSbrxxhtVv379fF8vNjZWrVq1cht78803XccXERGh22+/XQcOHHCbU9jn5M9YtWqVHA6HFixYoEceeURRUVGqUKGCbr755jw1SEXrhSTt3LlTffr0UbVq1RQcHKxGjRrp0UcfzTMvLS1NAwcOVHh4uMLCwjRo0CBlZGS4zUlMTFS7du0UHh6uihUrqlGjRh45dgD/wyUSAOXKsWPH1L17d91+++268847XT+enzt3ripWrKjhw4erYsWK+vzzzzVu3Dilp6e7XcEsyFtvvaVTp07pb3/7mxwOh5588kndcsst+umnn857tfmrr77Su+++q/vuu0+VKlXS888/r/j4eO3fv19VqlSRJH3zzTfq1q2batSooYkTJyo7O1uTJk1StWrV/vxJsebOnatBgwbpqquu0pQpU5SSkqLp06dr9erV+uabb1zLA+Lj47Vt2zbdf//9qlu3rlJTU5WYmKj9+/e7nnfp0kXVqlXT6NGjFR4err179+rdd98973k4ceKEHnzwwQKvsPfv319z5szRRx99pLZt26pv377q37+/1q9fr6uuuso1b9++fVqzZo1b7yZPnqzHHntMffr00V//+lcdOXJEM2bMUIcOHdyOTyr4c1KY48eP5xnz8/PLs6xi8uTJcjgcGjVqlFJTUzVt2jTFxcXp22+/VXBwsKSi9+K7775T+/bt5e/vryFDhqhu3br68ccf9eGHH2ry5Mlu79unTx/Vq1dPU6ZM0aZNm/Tyyy+revXqmjp1qiRp27ZtuvHGG3XZZZdp0qRJCgwM1A8//KDVq1ef99gB/AEGAMrA0KFDze//CurYsaORZGbPnp1nfkZGRp6xv/3tbyYkJMScOXPGNTZgwABTp04d1/M9e/YYSaZKlSrm+PHjrvH333/fSDIffviha2z8+PF5apJkAgICzA8//OAa27x5s5FkZsyY4Rq76aabTEhIiDl48KBrbPfu3cbPzy/Pa+ZnwIABpkKFCgVuz8rKMtWrVzfNmjUzp0+fdo1/9NFHRpIZN26cMcaYEydOGEnmqaeeKvC13nvvPSPJrF+//rx1/da0adOMJPPee+8VOOf48eNGkrnllluMMcacPHnSBAYGmhEjRrjNe/LJJ43D4TD79u0zxhizd+9e4+vrayZPnuw2b8uWLcbPz89tvLDPSX5y+5rfo1GjRq55K1euNJJMzZo1TXp6umt84cKFRpKZPn26MabovTDGmA4dOphKlSq5jjNXTk5Onvruvvtutzm9e/c2VapUcT1/7rnnjCRz5MiRIh03gOJhWQWAciUwMFCDBg3KM557xU6STp06paNHj6p9+/bKyMjQzp07z/u6ffv2VeXKlV3P27dvL0n66aefzrtvXFycGjRo4Hp+2WWXKTQ01LVvdna2PvvsM/Xq1UvR0dGueZdeeqm6d+9+3tcvig0bNig1NVX33XefgoKCXOM9evRQ48aN9fHHH0s6d54CAgK0atUqnThxIt/Xyr2q+dFHH8npdBa5hlOnTkmSKlWqVOCc3G3p6emSpNDQUHXv3l0LFy6UMcY1b8GCBWrbtq1q164tSXr33XeVk5OjPn366OjRo65HVFSUGjZsqJUrV7q9T0Gfk8K88847SkxMdHvMmTMnz7z+/fu7HeOtt96qGjVqaOnSpZKK3osjR47oiy++0N133+06zlz5LbW599573Z63b99ex44dc53L3L69//77xf7SKYDzIxwDKFdq1qypgICAPOPbtm1T7969FRYWptDQUFWrVs31Zb6TJ0+e93V/H05yg3JBAbKwfXP3z903NTVVp0+f1qWXXppnXn5jxbFv3z5JUqNGjfJsa9y4sWt7YGCgpk6dqk8++USRkZHq0KGDnnzySbfblXXs2FHx8fGaOHGiqlatqp49e2rOnDnKzMwstIbcwJgbkvOTX4Du27evDhw4oKSkJEnSjz/+qI0bN6pv376uObt375YxRg0bNlS1atXcHjt27FBqaqrb+xT0OSlMhw4dFBcX5/aIjY3NM69hw4Zuzx0Ohy699FLXmu2i9iL3f56aNWtWpPrO9xnt27evrrnmGv31r39VZGSkbr/9di1cuJCgDHgY4RhAufLbK8S50tLS1LFjR23evFmTJk3Shx9+qMTERNdazKKEA19f33zHf3s1syT2LQv/+Mc/9P3332vKlCkKCgrSY489ppiYGH3zzTeSzoW9xYsXKykpScOGDdPBgwd19913q2XLlvrll18KfN3c2+x99913Bc7J3dakSRPX2E033aSQkBAtXLhQkrRw4UL5+Pjotttuc83JycmRw+HQsmXL8lzdTUxM1H/+8x+398nvc+Ltzvc5Cw4O1hdffKHPPvtMd911l7777jv17dtXnTt3zvPFVADFRzgGUO6tWrVKx44d09y5c/Xggw/qxhtvVFxcnNsyibJUvXp1BQUF6YcffsizLb+x4qhTp44kadeuXXm27dq1y7U9V4MGDTRixAgtX75cW7duVVZWlp555hm3OW3bttXkyZO1YcMGzZs3T9u2bdP8+fMLrCH3LglvvfVWgWHs9ddfl3TuLhW5KlSooBtvvFGLFi1STk6OFixYoPbt27stQWnQoIGMMapXr16eq7txcXFq27btec6Q5+zevdvtuTFGP/zwg+suKEXtRe5dOrZu3eqx2nx8fNSpUyc9++yz2r59uyZPnqzPP/88z7ITAMVHOAZQ7uVeUfvtldqsrCy9+OKLZVWSG19fX8XFxWnJkiU6dOiQa/yHH37w2P1+W7VqperVq2v27Nluyx8++eQT7dixQz169JB07n6/Z86ccdu3QYMGqlSpkmu/EydO5Lnqffnll0tSoUsrQkJCNHLkSO3atSvfW5F9/PHHmjt3rrp27ZonzPbt21eHDh3Syy+/rM2bN7stqZCkW265Rb6+vpo4cWKe2owxOnbsWIF1edrrr7/utnRk8eLFOnz4sGv9eFF7Ua1aNXXo0EGvvvqq9u/f7/YexfmpQ3532yhK3wD8MdzKDUC5d/XVV6ty5coaMGCAHnjgATkcDr3xxhvlalnDhAkTtHz5cl1zzTX6+9//ruzsbM2cOVPNmjXTt99+W6TXcDqdeuKJJ/KMR0RE6L777tPUqVM1aNAgdezYUQkJCa7bh9WtW1cPPfSQJOn7779Xp06d1KdPHzVp0kR+fn567733lJKSottvv12S9Nprr+nFF19U79691aBBA506dUr//e9/FRoaqhtuuKHQGkePHq1vvvlGU6dOVVJSkuLj4xUcHKyvvvpKb775pmJiYvTaa6/l2e+GG25QpUqVNHLkSPn6+io+Pt5te4MGDfTEE09ozJgx2rt3r3r16qVKlSppz549eu+99zRkyBCNHDmySOexIIsXL873N+R17tzZ7VZwERERateunQYNGqSUlBRNmzZNl156qe655x5J537RTFF6IUnPP/+82rVrpyuvvFJDhgxRvXr1tHfvXn388cdF/lzkmjRpkr744gv16NFDderUUWpqql588UVdcsklateuXfFOCoC8yuQeGQAuegXdyq1p06b5zl+9erVp27atCQ4ONtHR0eaf//yn+fTTT40ks3LlSte8gm7llt+tzSSZ8ePHu54XdCu3oUOH5tm3Tp06ZsCAAW5jK1asMFdccYUJCAgwDRo0MC+//LIZMWKECQoKKuAs/M+AAQMKvN1YgwYNXPMWLFhgrrjiChMYGGgiIiJMv379zM8//+zafvToUTN06FDTuHFjU6FCBRMWFmbatGljFi5c6JqzadMmk5CQYGrXrm0CAwNN9erVzY033mg2bNhw3jqNMSY7O9vMmTPHXHPNNSY0NNQEBQWZpk2bmokTJ5pffvmlwP369etnJJm4uLgC57zzzjumXbt2pkKFCqZChQqmcePGZujQoWbXrl2uOYV9TvJT2K3cfvv5yb2V29tvv23GjBljqlevboKDg02PHj3y3IrNmPP3ItfWrVtN7969TXh4uAkKCjKNGjUyjz32WJ76fn+Ltjlz5hhJZs+ePcaYc5+vnj17mujoaBMQEGCio6NNQkKC+f7774t8LgCcn8OYcnTpBQAuML169dK2bdvyrGNF+bNq1Spdd911WrRokW699dayLgdAGWHNMQB4yOnTp92e7969W0uXLtW1115bNgUBAP4w1hwDgIfUr19fAwcOVP369bVv3z7NmjVLAQEB+uc//1nWpQEAiohwDAAe0q1bN7399ttKTk5WYGCgYmNj9a9//SvPL5UAAJRfrDkGAAAALNYcAwAAABbhGAAAALBYc+wBOTk5OnTokCpVqiSHw1HW5QAAAOB3jDE6deqUoqOj5eNT8PVhwrEHHDp0SLVq1SrrMgAAAHAeBw4c0CWXXFLgdsKxB1SqVEnSuZMdGhpaxtV4P6fTqeXLl6tLly7y9/cv63JQDPTQ+9FD70b/vB899Lz09HTVqlXLldsKQjj2gNylFKGhoYRjD3A6nQoJCVFoaCh/IXgpeuj96KF3o3/ejx6WnPMtgeULeQAAAIBFOAYAAAAswjEAAABgseYYAACUe8YYnT17VtnZ2WVdSqlwOp3y8/PTmTNnLppj/rN8fX3l5+f3p2+rSzgGAADlWlZWlg4fPqyMjIyyLqXUGGMUFRWlAwcO8DsU/oCQkBDVqFFDAQEBxX4NwjEAACi3cnJytGfPHvn6+io6OloBAQEXRVjMycnRL7/8oooVKxb6CytwjjFGWVlZOnLkiPbs2aOGDRsW+7wRjgEAQLmVlZWlnJwc1apVSyEhIWVdTqnJyclRVlaWgoKCCMdFFBwcLH9/f+3bt8917oqDsw0AAMo9AiKKwhOfEz5pAAAAgEU4BgAAACzCMQAAuOBl5xgl/XhM7397UEk/HlN2jinrkv6wunXratq0aUWev2rVKjkcDqWlpZVYTRcivpAHAAAuaMu2HtbED7fr8MkzrrEaYUEaf1MTdWtWw+Pvd767aYwfP14TJkz4w6+7fv16VahQocjzr776ah0+fFhhYWF/+L3+iFWrVum6667TiRMnFB4eXqLvVRoIxwAA4IK1bOth/f3NTfr9deLkk2f09zc3adadV3o8IB8+fNj17wsWLNC4ceO0a9cu11jFihVd/26MUXZ2tvz8zh/JqlWr9ofqCAgIUFRU1B/aByyrAAAAXsYYo4yss+d9nDrj1PgPtuUJxpJcYxM+2K5TZ5xFej1jirYUIyoqyvUICwuTw+FwPd+5c6cqVaqkTz75RC1btlRgYKC++uor/fjjj+rZs6ciIyNVsWJFtWnTRqtWrXJ73d8vq3A4HHr55ZfVu3dvhYSEqGHDhvrggw9c23+/rGLu3LkKDw/Xp59+qpiYGFWsWFHdunVzC/Nnz57VAw88oPDwcFWpUkWjRo3SgAED1KtXryIde35OnDih/v37q3LlygoJCVH37t21e/du1/Z9+/bppptuUuXKlVWhQgU1bdpUS5cude3br18/VatWTcHBwWrYsKHmzJlT7FqKgivHAADAq5x2ZqvJuE//9OsYScnpZ9R8wvIizd8+qatCAjwTnUaPHq2nn35a9evXV+XKlXXgwAHdcMMNmjx5sgIDA/Xaa68pISFBO3bsUN26dQt8nYkTJ+rJJ5/UU089pRkzZqhfv37at2+fIiIi8p2fkZGhp59+Wm+88YZ8fHx05513auTIkZo3b54kaerUqZo3b57mzJmjmJgYTZ8+XUuWLNF1111X7GMdOHCgdu/erQ8++EChoaEaNWqUbrjhBm3fvl3+/v4aOnSosrKy9MUXX6hChQravn276+r6Y489pu3bt+uTTz5R1apV9cMPP+j06dPFrqUoCMcAAAClbNKkSercubPreUREhFq0aOG2/Z133tGHH36o+++/v8DXGThwoBISEiRJ//rXv/T8889r3bp16tatW77znU6nZs+erQYNGkiShg0bpkmTJrm2z5gxQ2PGjFHv3r0lSTNnznRdxS2O3FC8evVqXX311ZKkefPmqVatWlqyZIluu+027d+/X/Hx8WrevLkkqX79+q799+/fryuuuEKtWrWSpEL/R8FTCMcAAMCrBPv7avukruedt27PcQ2cs/688+YOukqt6+V/pfX37+spuWEv1y+//KIJEybo448/1uHDh3X27FmdPn1a+/fvL/R1LrvsMte/V6hQQaGhoUpNTS1wfkhIiCsYS1KNGjVc80+ePKmUlBS1bt3atd3X11ctW7ZUTk7OHzq+XDt27JCfn5/atGnjGqtSpYoaNWqkHTt2SJIeeOAB/f3vf9fy5csVFxen+Ph413H9/e9/V3x8vDZt2qQuXbqoV69erpBdUlhzDAAAvIrD4VBIgN95H+0bVlONsCAVdO8Ih87dtaJ9w2pFer3z3YXij/j9XSdGjhyp9957T//617/05ZdfatOmTWrSpImysrIKfR1/f3/3Y3I4Cg2y+c0v6lrqkvLXv/5VP/30k+666y5t2bJFrVq10owZMyRJ3bt31759+/TQQw/p0KFD6tSpk0aOHFmi9RCOAQDABcnXx6HxNzWRpDwBOff5+JuayNfHc6G3uFavXq2BAweqd+/eat68uaKios571djTwsLCFBkZqfXr/3e1PTs7W5s2bSr2a8bExOjs2bNau3ata+zYsWPatWuXmjRp4hqrVauW7r33Xr377rsaMWKE/vvf/7q2VatWTQMGDNCbb76padOm6aWXXip2PUXBsgoAAHDB6tashmbdeWWe+xxHleB9joujYcOGevfdd3XTTTfJ4XBo7NixZXJF9/7779eUKVN06aWXqnHjxpoxY4ZOnDhRpKvmW7ZsUaVKlVzPHQ6HWrRooZ49e+qee+7Rf/7zH1WqVEmjR49WzZo11bNnT0nSP/7xD3Xv3l1/+ctfdOLECa1cuVIxMTGSpHHjxqlly5Zq2rSpMjMz9dFHH7m2lRTCMQAAuKB1a1ZDnZtEad2e40o9dUbVKwWpdb2IcnHFONezzz6ru+++W1dffbWqVq2qf/7znzpx4kSp1zFq1CglJyerf//+8vX11ZAhQ9S1a1f5+p5/vXWHDh3cnvv6+urs2bOaM2eOHnzwQd14443KyspShw4dtHTpUtcSj+zsbA0dOlQ///yzQkND1a1bNz333HOSzt2recyYMdq7d6+Cg4PVvn17zZ8/3/MH/hsOU9YLTS4A6enpCgsL08mTJxUaGlrW5Xg9p9OppUuX6oYbbsizNgregR56P3ro3S6k/p05c0Z79uxRvXr1FBQUVNbllJqcnBylp6crNDRUPj5ltwo2JydHMTEx6tOnjx5//PEyq6OoCvu8FDWvceUYAAAAks79Qo7ly5erY8eOyszM1MyZM7Vnzx7dcccdZV1aqeELeQAAAJAk+fj4aO7cubrqqqt0zTXXaMuWLfrss89KfJ1vecKVYwAAAEg6d9eI1atXl3UZZYorxwAAAIBFOAYAAOUe9w9AUXjic0I4BgAA5Vbu3TYyMjLKuBJ4g9zPyZ+5SwtrjgEAQLnl6+ur8PBwpaamSpJCQkI8+mucy6ucnBxlZWXpzJkzZXorN29hjFFGRoZSU1MVHh5epPsyF4RwDAAAyrWoqChJcgXki4ExRqdPn1ZwcPBF8T8DnhIeHu76vBQX4RgAAJRrDodDNWrUUPXq1eV0Osu6nFLhdDr1xRdfqEOHDl7/i1xKi7+//5+6YpyLcAwAALyCr6+vR8KPN8j91ctBQUGE41LGIhYAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYXheOX3jhBdWtW1dBQUFq06aN1q1bV+j8RYsWqXHjxgoKClLz5s21dOnSAufee++9cjgcmjZtmoerBgAAgDfwqnC8YMECDR8+XOPHj9emTZvUokULde3aVampqfnO//rrr5WQkKDBgwfrm2++Ua9evdSrVy9t3bo1z9z33ntPa9asUXR0dEkfBgAAAMoprwrHzz77rO655x4NGjRITZo00ezZsxUSEqJXX3013/nTp09Xt27d9PDDDysmJkaPP/64rrzySs2cOdNt3sGDB3X//fdr3rx58vf3L41DAQAAQDnkV9YFFFVWVpY2btyoMWPGuMZ8fHwUFxenpKSkfPdJSkrS8OHD3ca6du2qJUuWuJ7n5OTorrvu0sMPP6ymTZsWqZbMzExlZma6nqenp0uSnE6nnE5nUQ8JBcg9h5xL70UPvR899G70z/vRQ88r6rn0mnB89OhRZWdnKzIy0m08MjJSO3fuzHef5OTkfOcnJye7nk+dOlV+fn564IEHilzLlClTNHHixDzjy5cvV0hISJFfB4VLTEws6xLwJ9FD70cPvRv983700HMyMjKKNM9rwnFJ2Lhxo6ZPn65NmzbJ4XAUeb8xY8a4XZFOT09XrVq11KVLF4WGhpZEqRcVp9OpxMREde7cmWUuXooeej966N3on/ejh56X+5P+8/GacFy1alX5+voqJSXFbTwlJUVRUVH57hMVFVXo/C+//FKpqamqXbu2a3t2drZGjBihadOmae/evfm+bmBgoAIDA/OM+/v78wH2IM6n96OH3o8eejf65/3ooecU9Tx6zRfyAgIC1LJlS61YscI1lpOToxUrVig2NjbffWJjY93mS+d+PJE7/6677tJ3332nb7/91vWIjo7Www8/rE8//bTkDgYAAADlktdcOZak4cOHa8CAAWrVqpVat26tadOm6ddff9WgQYMkSf3791fNmjU1ZcoUSdKDDz6ojh076plnnlGPHj00f/58bdiwQS+99JIkqUqVKqpSpYrbe/j7+ysqKkqNGjUq3YMDAABAmfOqcNy3b18dOXJE48aNU3Jysi6//HItW7bM9aW7/fv3y8fnfxfDr776ar311lsaO3asHnnkETVs2FBLlixRs2bNyuoQAAAAUI55VTiWpGHDhmnYsGH5blu1alWesdtuu0233XZbkV+/oHXGAAAAuPB5zZpjAAAAoKQRjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGB5XTh+4YUXVLduXQUFBalNmzZat25dofMXLVqkxo0bKygoSM2bN9fSpUtd25xOp0aNGqXmzZurQoUKio6OVv/+/XXo0KGSPgwAAACUQ14VjhcsWKDhw4dr/Pjx2rRpk1q0aKGuXbsqNTU13/lff/21EhISNHjwYH3zzTfq1auXevXqpa1bt0qSMjIytGnTJj322GPatGmT3n33Xe3atUs333xzaR4WAAAAygmvCsfPPvus7rnnHg0aNEhNmjTR7NmzFRISoldffTXf+dOnT1e3bt308MMPKyYmRo8//riuvPJKzZw5U5IUFhamxMRE9enTR40aNVLbtm01c+ZMbdy4Ufv37y/NQwMAAEA54FfWBRRVVlaWNm7cqDFjxrjGfHx8FBcXp6SkpHz3SUpK0vDhw93GunbtqiVLlhT4PidPnpTD4VB4eHiBczIzM5WZmel6np6eLuncMg2n01mEo0Fhcs8h59J70UPvRw+9G/3zfvTQ84p6Lr0mHB89elTZ2dmKjIx0G4+MjNTOnTvz3Sc5OTnf+cnJyfnOP3PmjEaNGqWEhASFhoYWWMuUKVM0ceLEPOPLly9XSEjI+Q4FRZSYmFjWJeBPoofejx56N/rn/eih52RkZBRpnteE45LmdDrVp08fGWM0a9asQueOGTPG7Yp0enq6atWqpS5duhQaqlE0TqdTiYmJ6ty5s/z9/cu6HBQDPfR+9NC70T/vRw89L/cn/efjNeG4atWq8vX1VUpKitt4SkqKoqKi8t0nKiqqSPNzg/G+ffv0+eefnzfgBgYGKjAwMM+4v78/H2AP4nx6P3ro/eihd6N/3o8eek5Rz6PXfCEvICBALVu21IoVK1xjOTk5WrFihWJjY/PdJzY21m2+dO7HE7+dnxuMd+/erc8++0xVqlQpmQMAAABAuec1V44lafjw4RowYIBatWql1q1ba9q0afr11181aNAgSVL//v1Vs2ZNTZkyRZL04IMPqmPHjnrmmWfUo0cPzZ8/Xxs2bNBLL70k6VwwvvXWW7Vp0yZ99NFHys7Odq1HjoiIUEBAQNkcKAAAAMqEV4Xjvn376siRIxo3bpySk5N1+eWXa9myZa4v3e3fv18+Pv+7GH711Vfrrbfe0tixY/XII4+oYcOGWrJkiZo1ayZJOnjwoD744ANJ0uWXX+72XitXrtS1115bKscFAACA8sGrwrEkDRs2TMOGDct326pVq/KM3XbbbbrtttvynV+3bl0ZYzxZHgAAALyY16w5BgAAAEoa4RgAAACwCMcAAACARTgGAAAALMIxAAAAYBGOAQAAAItwDAAAAFiEYwAAAMAiHAMAAAAW4RgAAACwCMcAAACARTgGAAAALMIxAAAAYBGOAQAAAItwDAAAAFiEYwAAAMAiHAMAAAAW4RgAAACwCMcAAACARTgGAAAALMIxAAAAYBGOAQAAAItwDAAAAFiEYwAAAMAiHAMAAAAW4RgAAACwCMcAAACARTgGAAAALMIxAAAAYBGOAQAAAItwDAAAAFiEYwAAAMAiHAMAAAAW4RgAAACwCMcAAACARTgGAAAALMIxAAAAYBGOAQAAAItwDAAAAFiEYwAAAMAiHAMAAAAW4RgAAACwCMcAAACARTgGAAAALMIxAAAAYBGOAQAAAItwDAAAAFiEYwAAAMAiHAMAAAAW4RgAAACwCMcAAACARTgGAAAALMIxAAAAYBGOAQAAAItwDAAAAFiEYwAAAMAiHAMAAAAW4RgAAACwCMcAAACARTgGAAAALMIxAAAAYBGOAQAAAItwDAAAAFiEYwAAAMAiHAMAAABWscLxgQMH9PPPP7uer1u3Tv/4xz/00ksveawwAAAAoLQVKxzfcccdWrlypSQpOTlZnTt31rp16/Too49q0qRJHi0QAAAAKC3FCsdbt25V69atJUkLFy5Us2bN9PXXX2vevHmaO3euJ+sDAAAASk2xwrHT6VRgYKAk6bPPPtPNN98sSWrcuLEOHz7sueoAAACAUlSscNy0aVPNnj1bX375pRITE9WtWzdJ0qFDh1SlShWPFggAAACUlmKF46lTp+o///mPrr32WiUkJKhFixaSpA8++MC13AIAAADwNn7F2enaa6/V0aNHlZ6ersqVK7vGhwwZopCQEI8VBwAAAJSmYl05Pn36tDIzM13BeN++fZo2bZp27dql6tWre7RAAAAAoLQUKxz37NlTr7/+uiQpLS1Nbdq00TPPPKNevXpp1qxZHi3w91544QXVrVtXQUFBatOmjdatW1fo/EWLFqlx48YKCgpS8+bNtXTpUrftxhiNGzdONWrUUHBwsOLi4rR79+6SPAQAAACUU8UKx5s2bVL79u0lSYsXL1ZkZKT27dun119/Xc8//7xHC/ytBQsWaPjw4Ro/frw2bdqkFi1aqGvXrkpNTc13/tdff62EhAQNHjxY33zzjXr16qVevXpp69atrjlPPvmknn/+ec2ePVtr165VhQoV1LVrV505c6bEjgMAAADlU7HCcUZGhipVqiRJWr58uW655Rb5+Piobdu22rdvn0cL/K1nn31W99xzjwYNGqQmTZpo9uzZCgkJ0auvvprv/OnTp6tbt256+OGHFRMTo8cff1xXXnmlZs6cKencVeNp06Zp7Nix6tmzpy677DK9/vrrOnTokJYsWVJixwEAAIDyqVhfyLv00ku1ZMkS9e7dW59++qkeeughSVJqaqpCQ0M9WmCurKwsbdy4UWPGjHGN+fj4KC4uTklJSfnuk5SUpOHDh7uNde3a1RV89+zZo+TkZMXFxbm2h4WFqU2bNkpKStLtt9+e7+tmZmYqMzPT9Tw9PV3Sufs/O53OYh0f/if3HHIuvRc99H700LvRP+9HDz2vqOeyWOF43LhxuuOOO/TQQw/p+uuvV2xsrKRzV5GvuOKK4rzkeR09elTZ2dmKjIx0G4+MjNTOnTvz3Sc5OTnf+cnJya7tuWMFzcnPlClTNHHixDzjy5cv524dHpSYmFjWJeBPoofejx56N/rn/eih52RkZBRpXrHC8a233qp27drp8OHDrnscS1KnTp3Uu3fv4rykVxkzZozbFen09HTVqlVLXbp0KbEr5xcTp9OpxMREde7cWf7+/mVdDoqBHno/eujd6J/3o4eel/uT/vMpVjiWpKioKEVFRennn3+WJF1yySUl+gtAqlatKl9fX6WkpLiNp6SkKCoqqsAaC5uf+8+UlBTVqFHDbc7ll19eYC2BgYGuX5/9W/7+/nyAPYjz6f3oofejh96N/nk/eug5RT2PxfpCXk5OjiZNmqSwsDDVqVNHderUUXh4uB5//HHl5OQU5yXPKyAgQC1bttSKFSvc6lixYoVrWcfvxcbGus2Xzv14Ind+vXr1FBUV5TYnPT1da9euLfA1AQAAcOEq1pXjRx99VK+88or+/e9/65prrpEkffXVV5owYYLOnDmjyZMne7TIXMOHD9eAAQPUqlUrtW7dWtOmTdOvv/6qQYMGSZL69++vmjVrasqUKZKkBx98UB07dtQzzzyjHj16aP78+dqwYYNeeuklSZLD4dA//vEPPfHEE2rYsKHq1aunxx57TNHR0erVq1eJHAMAAADKr2KF49dee00vv/yybr75ZtfYZZddppo1a+q+++4rsXDct29fHTlyROPGjVNycrIuv/xyLVu2zPWFuv3798vH538Xw6+++mq99dZbGjt2rB555BE1bNhQS5YsUbNmzVxz/vnPf+rXX3/VkCFDlJaWpnbt2mnZsmUKCgoqkWMAAABA+VWscHz8+HE1btw4z3jjxo11/PjxP11UYYYNG6Zhw4blu23VqlV5xm677TbddtttBb6ew+HQpEmTNGnSJE+VCAAAAC9VrDXHLVq0cP0ijd+aOXOmLrvssj9dFAAAAFAWinXl+Mknn1SPHj302Wefub64lpSUpAMHDmjp0qUeLRAAAAAoLcW6ctyxY0d9//336t27t9LS0pSWlqZbbrlF27Zt0xtvvOHpGgEAAIBSUez7HEdHR+f54t3mzZv1yiuvuO4GAQAAAHiTYl05BgAAAC5EhGMAAADAIhwDAAAA1h9ac3zLLbcUuj0tLe3P1AIAAACUqT8UjsPCws67vX///n+qIAAAAKCs/KFwPGfOnJKqAwAAAChzrDkGAAAALMIxAAAAYBGOAQAAAItwDAAAAFiEYwAAAMAiHAMAAAAW4RgAAACwCMcAAACARTgGAAAALMIxAAAAYBGOAQAAAItwDAAAAFiEYwAAAMAiHAMAAAAW4RgAAACwCMcAAACARTgGAAAALMIxAAAAYBGOAQAAAItwDAAAAFiEYwAAAMAiHAMAAAAW4RgAAACwCMcAAACARTgGAAAALMIxAAAAYBGOAQAAAItwDAAAAFiEYwAAAMAiHAMAAAAW4RgAAACwCMcAAACARTgGAAAALMIxAAAAYBGOAQAAAItwDAAAAFiEYwAAAMAiHAMAAAAW4RgAAACwCMcAAACARTgGAAAALMIxAAAAYBGOAQAAAItwDAAAAFiEYwAAAMAiHAMAAAAW4RgAAACwCMcAAACARTgGAAAALMIxAAAAYBGOAQAAAItwDAAAAFiEYwAAAMAiHAMAAAAW4RgAAACwCMcAAACARTgGAAAALMIxAAAAYBGOAQAAAItwDAAAAFiEYwAAAMAiHAMAAAAW4RgAAACwvCYcHz9+XP369VNoaKjCw8M1ePBg/fLLL4Xuc+bMGQ0dOlRVqlRRxYoVFR8fr5SUFNf2zZs3KyEhQbVq1VJwcLBiYmI0ffr0kj4UAAAAlFNeE4779eunbdu2KTExUR999JG++OILDRkypNB9HnroIX344YdatGiR/u///k+HDh3SLbfc4tq+ceNGVa9eXW+++aa2bdumRx99VGPGjNHMmTNL+nAAAABQDvmVdQFFsWPHDi1btkzr169Xq1atJEkzZszQDTfcoKefflrR0dF59jl58qReeeUVvfXWW7r++uslSXPmzFFMTIzWrFmjtm3b6u6773bbp379+kpKStK7776rYcOGlfyBAQAAoFzxinCclJSk8PBwVzCWpLi4OPn4+Gjt2rXq3bt3nn02btwop9OpuLg411jjxo1Vu3ZtJSUlqW3btvm+18mTJxUREVFoPZmZmcrMzHQ9T09PlyQ5nU45nc4/dGzIK/ccci69Fz30fvTQu9E/70cPPa+o59IrwnFycrKqV6/uNubn56eIiAglJycXuE9AQIDCw8PdxiMjIwvc5+uvv9aCBQv08ccfF1rPlClTNHHixDzjy5cvV0hISKH7ougSExPLugT8SfTQ+9FD70b/vB899JyMjIwizSvTcDx69GhNnTq10Dk7duwolVq2bt2qnj17avz48erSpUuhc8eMGaPhw4e7nqenp6tWrVrq0qWLQkNDS7rUC57T6VRiYqI6d+4sf3//si4HxUAPvR899G70z/vRQ8/L/Un/+ZRpOB4xYoQGDhxY6Jz69esrKipKqampbuNnz57V8ePHFRUVle9+UVFRysrKUlpamtvV45SUlDz7bN++XZ06ddKQIUM0duzY89YdGBiowMDAPOP+/v58gD2I8+n96KH3o4fejf55P3roOUU9j2UajqtVq6Zq1aqdd15sbKzS0tK0ceNGtWzZUpL0+eefKycnR23atMl3n5YtW8rf318rVqxQfHy8JGnXrl3av3+/YmNjXfO2bdum66+/XgMGDNDkyZM9cFQAAADwVl5xK7eYmBh169ZN99xzj9atW6fVq1dr2LBhuv322113qjh48KAaN26sdevWSZLCwsI0ePBgDR8+XCtXrtTGjRs1aNAgxcbGur6Mt3XrVl133XXq0qWLhg8fruTkZCUnJ+vIkSNldqwAAAAoO17xhTxJmjdvnoYNG6ZOnTrJx8dH8fHxev75513bnU6ndu3a5bbY+rnnnnPNzczMVNeuXfXiiy+6ti9evFhHjhzRm2++qTfffNM1XqdOHe3du7dUjgsAAADlh9eE44iICL311lsFbq9bt66MMW5jQUFBeuGFF/TCCy/ku8+ECRM0YcIET5YJAAAAL+YVyyoAAACA0kA4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgOU14fj48ePq16+fQkNDFR4ersGDB+uXX34pdJ8zZ85o6NChqlKliipWrKj4+HilpKTkO/fYsWO65JJL5HA4lJaWVgJHAAAAgPLOa8Jxv379tG3bNiUmJuqjjz7SF198oSFDhhS6z0MPPaQPP/xQixYt0v/93//p0KFDuuWWW/KdO3jwYF122WUlUToAAAC8hFeE4x07dmjZsmV6+eWX1aZNG7Vr104zZszQ/PnzdejQoXz3OXnypF555RU9++yzuv7669WyZUvNmTNHX3/9tdasWeM2d9asWUpLS9PIkSNL43AAAABQTvmVdQFFkZSUpPDwcLVq1co1FhcXJx8fH61du1a9e/fOs8/GjRvldDoVFxfnGmvcuLFq166tpKQktW3bVpK0fft2TZo0SWvXrtVPP/1UpHoyMzOVmZnpep6eni5JcjqdcjqdxTpG/E/uOeRcei966P3ooXejf96PHnpeUc+lV4Tj5ORkVa9e3W3Mz89PERERSk5OLnCfgIAAhYeHu41HRka69snMzFRCQoKeeuop1a5du8jheMqUKZo4cWKe8eXLlyskJKRIr4HzS0xMLOsS8CfRQ+9HD70b/fN+9NBzMjIyijSvTMPx6NGjNXXq1ELn7Nixo8Tef8yYMYqJidGdd975h/cbPny463l6erpq1aqlLl26KDQ01NNlXnScTqcSExPVuXNn+fv7l3U5KAZ66P3ooXejf96PHnpe7k/6z6dMw/GIESM0cODAQufUr19fUVFRSk1NdRs/e/asjh8/rqioqHz3i4qKUlZWltLS0tyuHqekpLj2+fzzz7VlyxYtXrxYkmSMkSRVrVpVjz76aL5XhyUpMDBQgYGBecb9/f35AHsQ59P70UPvRw+9G/3zfvTQc4p6Hss0HFerVk3VqlU777zY2FilpaVp48aNatmypaRzwTYnJ0dt2rTJd5+WLVvK399fK1asUHx8vCRp165d2r9/v2JjYyVJ77zzjk6fPu3aZ/369br77rv15ZdfqkGDBn/28AAAAOBlvGLNcUxMjLp166Z77rlHs2fPltPp1LBhw3T77bcrOjpaknTw4EF16tRJr7/+ulq3bq2wsDANHjxYw4cPV0REhEJDQ3X//fcrNjbW9WW83wfgo0ePut7v92uVAQAAcOHzinAsSfPmzdOwYcPUqVMn+fj4KD4+Xs8//7xru9Pp1K5du9wWWz/33HOuuZmZmeratatefPHFsigfAAAAXsBrwnFERITeeuutArfXrVvXtWY4V1BQkF544QW98MILRXqPa6+9Ns9rAAAA4OLhFb8EBAAAACgNhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGARjgEAAACLcAwAAABYhGMAAADAIhwDAAAAFuEYAAAAsAjHAAAAgEU4BgAAACzCMQAAAGD5lXUBFwJjjCQpPT29jCu5MDidTmVkZCg9PV3+/v5lXQ6KgR56P3ro3eif96OHnpeb03JzW0EIxx5w6tQpSVKtWrXKuBIAAAAU5tSpUwoLCytwu8OcLz7jvHJycnTo0CFVqlRJDoejrMvxeunp6apVq5YOHDig0NDQsi4HxUAPvR899G70z/vRQ88zxujUqVOKjo6Wj0/BK4u5cuwBPj4+uuSSS8q6jAtOaGgofyF4OXro/eihd6N/3o8eelZhV4xz8YU8AAAAwCIcAwAAABbhGOVOYGCgxo8fr8DAwLIuBcVED70fPfRu9M/70cOywxfyAAAAAIsrxwAAAIBFOAYAAAAswjEAAABgEY4BAAAAi3CMUnf8+HH169dPoaGhCg8P1+DBg/XLL78Uus+ZM2c0dOhQValSRRUrVlR8fLxSUlLynXvs2DFdcsklcjgcSktLK4EjQEn0cPPmzUpISFCtWrUUHBysmJgYTZ8+vaQP5aLxwgsvqG7dugoKClKbNm20bt26QucvWrRIjRs3VlBQkJo3b66lS5e6bTfGaNy4capRo4aCg4MVFxen3bt3l+QhXPQ82UOn06lRo0apefPmqlChgqKjo9W/f38dOnSopA/joubpP4e/de+998rhcGjatGkervoiZIBS1q1bN9OiRQuzZs0a8+WXX5pLL73UJCQkFLrPvffea2rVqmVWrFhhNmzYYNq2bWuuvvrqfOf27NnTdO/e3UgyJ06cKIEjQEn08JVXXjEPPPCAWbVqlfnxxx/NG2+8YYKDg82MGTNK+nAuePPnzzcBAQHm1VdfNdu2bTP33HOPCQ8PNykpKfnOX716tfH19TVPPvmk2b59uxk7dqzx9/c3W7Zscc3597//bcLCwsySJUvM5s2bzc0332zq1atnTp8+XVqHdVHxdA/T0tJMXFycWbBggdm5c6dJSkoyrVu3Ni1btizNw7qolMSfw1zvvvuuadGihYmOjjbPPfdcCR/JhY9wjFK1fft2I8msX7/eNfbJJ58Yh8NhDh48mO8+aWlpxt/f3yxatMg1tmPHDiPJJCUluc198cUXTceOHc2KFSsIxyWkpHv4W/fdd5+57rrrPFf8Rap169Zm6NChrufZ2dkmOjraTJkyJd/5ffr0MT169HAba9Omjfnb3/5mjDEmJyfHREVFmaeeesq1PS0tzQQGBpq33367BI4Anu5hftatW2ckmX379nmmaLgpqR7+/PPPpmbNmmbr1q2mTp06hGMPYFkFSlVSUpLCw8PVqlUr11hcXJx8fHy0du3afPfZuHGjnE6n4uLiXGONGzdW7dq1lZSU5Brbvn27Jk2apNdff10+Pny0S0pJ9vD3Tp48qYiICM8VfxHKysrSxo0b3c69j4+P4uLiCjz3SUlJbvMlqWvXrq75e/bsUXJystucsLAwtWnTptB+onhKoof5OXnypBwOh8LDwz1SN/6npHqYk5Oju+66Sw8//LCaNm1aMsVfhEgQKFXJycmqXr2625ifn58iIiKUnJxc4D4BAQF5/sKOjIx07ZOZmamEhAQ99dRTql27donUjnNKqoe/9/XXX2vBggUaMmSIR+q+WB09elTZ2dmKjIx0Gy/s3CcnJxc6P/eff+Q1UXwl0cPfO3PmjEaNGqWEhASFhoZ6pnC4lFQPp06dKj8/Pz3wwAOeL/oiRjiGR4wePVoOh6PQx86dO0vs/ceMGaOYmBjdeeedJfYeF7qy7uFvbd26VT179tT48ePVpUuXUnlP4GLldDrVp08fGWM0a9assi4HRbRx40ZNnz5dc+fOlcPhKOtyLih+ZV0ALgwjRozQwIEDC51Tv359RUVFKTU11W387NmzOn78uKKiovLdLyoqSllZWUpLS3O78piSkuLa5/PPP9eWLVu0ePFiSee+SS9JVatW1aOPPqqJEycW88guHmXdw1zbt29Xp06dNGTIEI0dO7ZYx4L/qVq1qnx9ffPc3SW/c58rKiqq0Pm5/0xJSVGNGjXc5lx++eUerB5SyfQwV24w3rdvnz7//HOuGpeQkujhl19+qdTUVLeflmZnZ2vEiBGaNm2a9u7d69mDuJiU9aJnXFxyv8y1YcMG19inn35apC9zLV682DW2c+dOty9z/fDDD2bLli2ux6uvvmokma+//rrAbwKjeEqqh8YYs3XrVlO9enXz8MMPl9wBXIRat25thg0b5nqenZ1tatasWegXgW688Ua3sdjY2DxfyHv66add20+ePMkX8kqQp3tojDFZWVmmV69epmnTpiY1NbVkCoeLp3t49OhRt//ubdmyxURHR5tRo0aZnTt3ltyBXAQIxyh13bp1M1dccYVZu3at+eqrr0zDhg3dbgP2888/m0aNGpm1a9e6xu69915Tu3Zt8/nnn5sNGzaY2NhYExsbW+B7rFy5krtVlKCS6OGWLVtMtWrVzJ133mkOHz7sevAf7T9v/vz5JjAw0MydO9ds377dDBkyxISHh5vk5GRjjDF33XWXGT16tGv+6tWrjZ+fn3n66afNjh07zPjx4/O9lVt4eLh5//33zXfffWd69uzJrdxKkKd7mJWVZW6++WZzySWXmG+//dbtz1xmZmaZHOOFriT+HP4ed6vwDMIxSt2xY8dMQkKCqVixogkNDTWDBg0yp06dcm3fs2ePkWRWrlzpGjt9+rS57777TOXKlU1ISIjp3bu3OXz4cIHvQTguWSXRw/HjxxtJeR516tQpxSO7cM2YMcPUrl3bBAQEmNatW5s1a9a4tnXs2NEMGDDAbf7ChQvNX/7yFxMQEGCaNm1qPv74Y7ftOTk55rHHHjORkZEmMDDQdOrUyezatas0DuWi5cke5v4Zze/x2z+38CxP/zn8PcKxZziMsYszAQAAgIscd6sAAAAALMIxAAAAYBGOAQAAAItwDAAAAFiEYwAAAMAiHAMAAAAW4RgAAACwCMcAAACARTgGAHiMw+HQkiVLyroMACg2wjEAXCAGDhwoh8OR59GtW7eyLg0AvIZfWRcAAPCcbt26ac6cOW5jgYGBZVQNAHgfrhwDwAUkMDBQUVFRbo/KlStLOrfkYdasWerevbuCg4NVv359LV682G3/LVu26Prrr1dwcLCqVKmiIUOG6JdffnGb8+qrr6pp06YKDAxUjRo1NGzYMLftR48eVe/evRUSEqKGDRvqgw8+KNmDBgAPIhwDwEXkscceU3x8vDZv3qx+/frp9ttv144dOyRJv/76q7p27arKlStr/fr1WrRokT777DO38Dtr1iwNHTpUQ4YM0ZYtW/TBBx/o0ksvdXuPiRMnqk+fPvruu+90ww03qF+/fjp+/HipHicAFJfDGGPKuggAwJ83cOBAvfnmmwoKCnIbf+SRR/TII4/I4XDo3nvv1axZs1zb2rZtqyuvvFIvvvii/vvf/2rUqFE6cOCAKlSoIElaunSpbrrpJh06dEiRkZGqWbOmBg0apCeeeCLfGhwOh8aOHavHH39c0rnAXbFiRX3yySesfQbgFVhzDAAXkOuuu84t/EpSRESE699jY2PdtsXGxurbb7+VJO3YsUMtWrRwBWNJuuaaa5STk6Ndu3bJ4XDo0KFD6tSpU6E1XHbZZa5/r1ChgkJDQ5WamlrcQwKAUkU4BoALSIUKFfIsc/CU4ODgIs3z9/d3e+5wOJSTk1MSJQGAx7HmGAAuImvWrMnzPCYmRpIUExOjzZs369dff3VtX716tXx8fNSoUSNVqlRJdevW1YoVK0q1ZgAoTVw5BoALSGZmppKTk93G/Pz8VLVqVUnSokWL1KpVK7Vr107z5s3TunXr9Morr0iS+vXrp/Hjx2vAgAGaMGGCjhw5ovvvv1933XWXIiMjJUkTJkzQvffeq+rVq6t79+46deqUVq9erfvvv790DxQASgjhGAAuIMuWLVONGjXcxho1aqSdO3dKOncnifnz5+u+++5TjRo19Pbbb6tJkyaSpJCQEH366ad68MEHddVVVykkJETx8fF69tlnXa81YMAAnTlzRs8995xGjhypqlWr6tZbby29AwSAEsbdKgDgIuFwOPTee++pV69eZV0KAJRbrDkGAAAALMIxAAAAYLHmGAAuEqyiA4Dz48oxAAAAYBGOAQAAAItwDAAAAFiEYwAAAMAiHAMAAAAW4RgAAACwCMcAAACARTgGAAAArP8HgRwP6u2c2mkAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXwnmpjlu5sa"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhz13JznTujh"
      },
      "source": [
        "Exciting! Now is the time to test your model.   \n",
        "\n",
        "    May the gradients be always in your favor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "BHDYRUoq54Fk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b97b9728-1656-4c28-a979-b81c0e647253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: 0.1700, Accuracy: 3816/4000 (95%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "output = net.predict(x_test[:4000,] ,y_test[:4000] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9HEZ6ElvVVj"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "## 3. Benchmarking against PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-0UWnaYUNz7"
      },
      "source": [
        "How well your model performs against a similar-architecture PyTorch model?   \n",
        "It is time to find out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "R2TeiObsnBr1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h8cCoV3ZSkt"
      },
      "source": [
        "#### Prepare the data as tensors using PyTorch DataLoader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "1rqwlzUIvFCZ"
      },
      "outputs": [],
      "source": [
        "t_train =  TensorDataset(torch.Tensor(x_train),torch.Tensor(y_train))\n",
        "t_test =  TensorDataset(torch.Tensor(x_test),torch.Tensor(y_test))\n",
        "train_loader = torch.utils.data.DataLoader(dataset=t_train, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=t_test, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for images, labels in train_loader:\n",
        "#     print(images.shape, labels.shape)  # Verify shapes\n"
      ],
      "metadata": {
        "id": "ce59XY5XqXRS"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ngm-Gv_UsCV"
      },
      "source": [
        "Define a `PyTorchNet` class with an identical architecture you used in your home-made network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJBs2JsyNxid"
      },
      "source": [
        "3.1 (10 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "4Ed2P1LmUpgS"
      },
      "outputs": [],
      "source": [
        "class PyTorchNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PyTorchNet, self).__init__()\n",
        "        input_size = 784  # 28x28 (flattened input size)\n",
        "        num_classes = 10\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.activ1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.activ2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "        self.activ3 = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.activ1(self.fc1(x))\n",
        "        x = self.activ2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "fG-8BEdDlL4L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bfbeb65-1034-48a8-c292-0bedb970d6d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [500/938], Loss: 0.0082\n",
            "Epoch [2/20], Step [500/938], Loss: 0.0102\n",
            "Epoch [3/20], Step [500/938], Loss: 0.0121\n",
            "Epoch [4/20], Step [500/938], Loss: 0.0106\n",
            "Epoch [5/20], Step [500/938], Loss: 0.0060\n",
            "Epoch [6/20], Step [500/938], Loss: 0.0077\n",
            "Epoch [7/20], Step [500/938], Loss: 0.0048\n",
            "Epoch [8/20], Step [500/938], Loss: 0.0056\n",
            "Epoch [9/20], Step [500/938], Loss: 0.0095\n",
            "Epoch [10/20], Step [500/938], Loss: 0.0036\n",
            "Epoch [11/20], Step [500/938], Loss: 0.0068\n",
            "Epoch [12/20], Step [500/938], Loss: 0.0072\n",
            "Epoch [13/20], Step [500/938], Loss: 0.0029\n",
            "Epoch [14/20], Step [500/938], Loss: 0.0050\n",
            "Epoch [15/20], Step [500/938], Loss: 0.0021\n",
            "Epoch [16/20], Step [500/938], Loss: 0.0054\n",
            "Epoch [17/20], Step [500/938], Loss: 0.0037\n",
            "Epoch [18/20], Step [500/938], Loss: 0.0032\n",
            "Epoch [19/20], Step [500/938], Loss: 0.0072\n",
            "Epoch [20/20], Step [500/938], Loss: 0.0047\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Train the model\n",
        "num_epochs = 20\n",
        "pt_learning_rate = 0.01\n",
        "pt_network = PyTorchNet()\n",
        "optimizer = torch.optim.Adam(pt_network.parameters(), lr=pt_learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = pt_network(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # A handy printout:\n",
        "        if (i + 1) % 500 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3rFfBfaV3Gt"
      },
      "source": [
        "Evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "RsfDSk2IrXst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb340da-d8f1-4b32-89da-07bbc4b61b7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: 0.0002, Accuracy: 9466/10000 (95%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pt_network.eval()\n",
        "test_losses = []\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = pt_network(data)\n",
        "        test_loss += criterion(output, target,)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.max(1,keepdim=True)[1]).sum()\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_losses.append(test_loss)\n",
        "print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "  test_loss, correct, len(test_loader.dataset),\n",
        "  100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyKXHGW3XsAN"
      },
      "source": [
        "3.2 (10 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI13KBVgrBkc"
      },
      "source": [
        "Time for some questions:\n",
        "1. Which one of the models performed better? Why?\n",
        "2. Which one of the models performed faster? Why?  \n",
        "3. What would you change in your network's architecture?   \n",
        "4. What would you change in your model's solution algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb6AMtbuXvSR"
      },
      "source": [
        "Write your solutions here:\n",
        "1. To my suprise the Pytorch version reached better performence in terms of accuracy when predicting on the test set. This could probably be attributed to my regularization. The average loss of pytorch is much better.\n",
        "2. Again, pytorch is better since the backend is optimized with features like:\n",
        "- type checking\n",
        "- parallelization, works on GPU\n",
        "- optimization algorithms allow faster convergence\n",
        "- uses mini batches\n",
        "\n",
        "3. In terms or architecture -\n",
        "4. Algorithmicly - I want my network to work faster.\n",
        "  - I already implemented regularization of the sigmoid (clipping the function)\n",
        "In the upcoming iterations I want to add:\n",
        "  - Learning rate decay - gradualy reducing the learning rate along the iterations\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmJYjygWrCAg"
      },
      "source": [
        "## 4. The Network Wars!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_bdbde5Wj-y"
      },
      "source": [
        "Here is your chance to play with your model's architecture in order to break your own benchmark set eariler.  \n",
        "You can add/remove layers, play with their sizes, types, etc.   \n",
        "You can add a new loss if you wish, or anything else that will fairly give your model an advantage over base.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG2hhaeyN59O"
      },
      "source": [
        "4.1 (15 pts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define network architecture\n",
        "net_update = MyNetwork()\n",
        "\n",
        "# Layer 1: Fully connected with ReLU\n",
        "net_update.add(Affine_Layer(input_size=784, output_size=128))\n",
        "net_update.add(ActivationLayer(activation=relu, activation_grad=relu_grad))\n",
        "\n",
        "# Layer 2: Fully connected with ReLU\n",
        "net_update.add(Affine_Layer(input_size=128, output_size=64))\n",
        "net_update.add(ActivationLayer(activation=relu, activation_grad=relu_grad))\n",
        "\n",
        "# Output Layer: Fully connected with Softmax\n",
        "net_update.add(Affine_Layer(input_size=64, output_size=10))\n",
        "net_update.add(ActivationLayer(activation=softmax, activation_grad=None))  # Final softmax\n"
      ],
      "metadata": {
        "id": "rc2FalehlVZ3"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### SOLUTION REQUIRED (in `predict`) ####\n",
        "\n",
        "class MyNetwork:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.loss = None\n",
        "        self.loss_grad = None\n",
        "        self.loss_history = []\n",
        "\n",
        "    # add layer to network\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    # set loss to use\n",
        "    def use_loss(self, loss, loss_grad):\n",
        "        self.loss = loss\n",
        "        self.loss_grad = loss_grad\n",
        "\n",
        "\n",
        "    # train the network\n",
        "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
        "        # sample dimension first\n",
        "        samples = len(x_train)\n",
        "        decay = 0.01\n",
        "        # training loop\n",
        "        for i in range(epochs):\n",
        "            err = 0\n",
        "            for j in range(samples):\n",
        "                # forward propagation\n",
        "                output = x_train[j]\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forward_propagation(output)\n",
        "\n",
        "                # compute loss (for display purpose only)\n",
        "                err += self.loss(y_train[j], output)\n",
        "\n",
        "                # backward propagation\n",
        "                grad = self.loss_grad(y_train[j], output)\n",
        "                for layer in reversed(self.layers):\n",
        "                    learning_rate = learning_rate\n",
        "                    grad = layer.backward_propagation(grad, learning_rate)\n",
        "\n",
        "            # calculate average error on all samples\n",
        "            err /= samples\n",
        "            self.loss_history.append(err)  # Store loss\n",
        "            print('Training epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
        "\n",
        "\n",
        "    # predict output for given input\n",
        "    def predict(self, x_test,y_test=np.array([])):\n",
        "        if y_test.size:\n",
        "           assert len(x_test)==len(y_test) # if Y is given\n",
        "        # sample dimension first\n",
        "        samples = len(x_test)\n",
        "        result = []\n",
        "        loss = 0\n",
        "        correct = 0\n",
        "        # run network over all samples\n",
        "        for i in range(samples):\n",
        "            # forward propagation\n",
        "            output = x_test[i]\n",
        "            for layer in self.layers:\n",
        "                output = layer.forward_propagation(output)\n",
        "            result.append(output)\n",
        "            # ONLY IF LABELS ARE GIVEN (Y):\n",
        "            if y_test.size:\n",
        "                # Evaluate the output against Y,\n",
        "                # calculate loss against Y, add to `loss`:\n",
        "                loss += mse(y_test, result)# FILL IN THE MISSING CODE\n",
        "                target = y_test[i]\n",
        "                # Evaluate the label of the output against real, and if identical (for categorical)\n",
        "                if np.argmax(output) == np.argmax(target):\n",
        "                   correct += 1\n",
        "        if y_test.size:\n",
        "            mean_loss = loss/samples\n",
        "\n",
        "            print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.\n",
        "                  format(mean_loss, correct, samples,100. * correct / samples))\n",
        "\n",
        "        return result\n"
      ],
      "metadata": {
        "id": "ccAC5afGguaw"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_history = []\n",
        "net_update.use_loss(mse, mse_grad)\n",
        "epoch_num = 20\n",
        "lr = 0.01\n",
        "t1 = time.time()\n",
        "net.fit(x_train, y_train, epochs=epoch_num, learning_rate=lr)\n",
        "print(f\"Total process time: {round(time.time() - t1,3)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE0_QvkDpGh_",
        "outputId": "2b26cd68-57d7-453f-8649-9ba580353d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 1/20   error=0.001889\n",
            "Training epoch 2/20   error=0.001836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = net_update.predict(x_test[:4000,] ,y_test[:4000] )"
      ],
      "metadata": {
        "id": "uqr28Iwz3Qbo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}